{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tX4qkH_B_LWe"
   },
   "source": [
    "# Diffusion Illusions: Twisting Squares\n",
    "\n",
    "Hi! Welcome to the official colab demo for our demo \"Diffusion Illusions: Hiding Images in Plain Sight\"\n",
    "\n",
    "By Ryan Burgert, Xiang Li, Abraham Leite, Kanchana Ranasinghe and Michael Ryoo from Stony Brook University\n",
    "\n",
    "Visit our project website: [diffusionillusions.com](https://ryanndagreat.github.io/Diffusion-Illusions/)\n",
    "\n",
    "This project was inspired by our paper \"Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors\". The Peekaboo project website: [https://ryanndagreat.github.io/peekaboo/](https://ryanndagreat.github.io/peekaboo/)\n",
    "\n",
    "Instructions:\n",
    "\n",
    "0. Go to the Runtime menu, and make sure this notebook is using GPU!\n",
    "1. Run the top 2 code cells (one cleans colab's junk and downloads the source code, while the other installs python packages)\n",
    "2. Click 'Runtime', then 'Restart Runtime'. You need to do this the first time you open this notebook to avoid weird random errors from the pip installations.\n",
    "3. Run code cells to load stable diffusion. The first time you run it it will take a few minutes to download; subsequent times won't take long at all though.\n",
    "4. Run all the cells below that, and customize prompt_a and prompt_b!\n",
    "5. Take the result top_image and bottom_image, print them out, and shine a backlight through them like shown in the Diffusion Illusion website (link above!)\n",
    "\n",
    "I may also create a YouTube tutorial if there's interest. Let me know if this would be helpful!\n",
    "\n",
    "This notebook was written by Ryan Burgert. Feel free to reach out to me at rburgert@cs.stonybrook.edu if you have any questions! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "80riJZ7f_LyL"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -d \".git\" ]; then \n",
    "    rm -rf * .*; #Get rid of Colab's default junk files\n",
    "    git clone -b master https://github.com/RyannDaGreat/Diffusion-Illusions .\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGDndq0_bch2",
    "outputId": "81a50af6-5861-4990-dfe5-871ac1682d28"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade -r requirements.txt\n",
    "%pip install rp --upgrade\n",
    "# You may need to restart the runtime after installing these\n",
    "# I'm not sure why this helps, but all sorts of weird random errors pop up in Colab if you don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7A1Fw50eDjJ",
    "outputId": "37ceb472-da12-4c74-a76b-a89f39a3a14d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SET YOUR PROMPTS HERE\n",
    "\n",
    "prompt_a = \"Hatsune Miku\"\n",
    "prompt_b = \"A dog's face photograph 4k\"\n",
    "\n",
    "prompt_a=''\n",
    "prompt_b=''\n",
    "\n",
    "NUMBER_OF_SQUARES = 4 # A 4x4 twisting squares illusion\n",
    "\n",
    "#Optional: Specify what you DON'T want to see\n",
    "negative_prompt = 'blurry ugly'\n",
    "\n",
    "print()\n",
    "print('Negative prompt:',repr(negative_prompt))\n",
    "print()\n",
    "print('Chosen prompts:')\n",
    "print('    prompt_a =', repr(prompt_a)) #This will be right-side up\n",
    "print('    prompt_b =', repr(prompt_b)) #This will be upside-down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FAtxvveUbquu"
   },
   "outputs": [],
   "source": [
    "from rp import *\n",
    "import numpy as np\n",
    "import rp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import source.stable_diffusion as sd\n",
    "from easydict import EasyDict\n",
    "from source.learnable_textures import LearnableImageFourier\n",
    "from source.stable_diffusion_labels import NegativeLabel\n",
    "from itertools import chain\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "ec572d79f75648e69c18f8fbb8445abe",
      "8a95ebc987364a3cbbc27866ff8c964b",
      "13511603d504480489e9de201dd85211",
      "f72921cdc424455e8d2e86b0ea07a29b",
      "df5753e3d2b54078a5ff58f1f0f5de78",
      "f4b3425062c041b3aa5db68a73ea0a01",
      "eb7e46a3bff547ba8d07ebc87eddd9a0",
      "875b8337c4304ae0b9ea63eb7435d3ba",
      "f767f331f3af45d4a3814a5447d29004",
      "19d10e0e23fa460c8ba196c1691ecef3",
      "38124b85cb824de2a2640dfede710372"
     ]
    },
    "id": "wi9Y9Zp5ejSP",
    "outputId": "b79dad35-3726-4efb-f640-dfb3f27df788"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "\n",
    "if 's' not in dir():\n",
    "    #You can select the original Stable Diffusion 1.5 or some dreambooth of it\n",
    "    model_name=\"CompVis/stable-diffusion-v1-4\"\n",
    "    model_name=\"runwayml/stable-diffusion-v1-5\"\n",
    "    # model_name=\"nitrosocke/Arcane-Diffusion\"\n",
    "    gpu=rp.select_torch_device()\n",
    "    s=sd.StableDiffusion(gpu,model_name)\n",
    "device=s.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HL_pjdcFekG6"
   },
   "outputs": [],
   "source": [
    "label_a = NegativeLabel(prompt_a,negative_prompt)\n",
    "label_b = NegativeLabel(prompt_b,negative_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "[transforms.Resize((512,512)),\n",
    "transforms.ToTensor()])\n",
    "\n",
    "def load_image(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "def transform_image(img):\n",
    "    if img is None:\n",
    "        return None\n",
    "    return transform(img).reshape(1,3,512,512).to(device)\n",
    "\n",
    "img_a='./img/miko.png'\n",
    "img_b='./img/shogun.png'\n",
    "img_prompt_a = load_image(img_a) if img_a else None\n",
    "img_prompt_b = load_image(img_b) if img_b else None\n",
    "\n",
    "img_prompts=[transform_image(img_prompt_a),transform_image(img_prompt_b)]\n",
    "\n",
    "# display the image\n",
    "rp.display_image(img_prompt_a)\n",
    "rp.display_image(img_prompt_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6LoGGFTkelJJ"
   },
   "outputs": [],
   "source": [
    "#Image Parametrization and Initialization (this section takes vram)\n",
    "\n",
    "#Select Learnable Image Size (this has big VRAM implications!):\n",
    "#Note: We use implicit neural representations for better image quality\n",
    "#They're previously used in our paper \"TRITON: Neural Neural Textures make Sim2Real Consistent\" (see tritonpaper.github.io)\n",
    "# ... and that representation is based on Fourier Feature Networks (see bmild.github.io/fourfeat)\n",
    "learnable_image_maker = lambda: LearnableImageFourier(height=256,width=256,num_features=256,hidden_dim=256,scale=10).to(s.device);SIZE=256\n",
    "\n",
    "image=learnable_image_maker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "44f6FT72ems4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def rotate_tiles(image, num_divisions=NUMBER_OF_SQUARES):\n",
    "    image=as_torch_image(image)\n",
    "    # Assuming image is a tensor of shape (num_channels, height, width)\n",
    "    num_channels, height, width = image.shape\n",
    "\n",
    "    tile_size=width//num_divisions\n",
    "    \n",
    "    # Calculate the number of tiles in each dimension\n",
    "    tiles_x = width // tile_size\n",
    "    tiles_y = height // tile_size\n",
    "\n",
    "    # Initialize an output tensor\n",
    "    output = torch.zeros_like(image)\n",
    "\n",
    "    for x in range(tiles_x):\n",
    "        for y in range(tiles_y):\n",
    "            # Extract the tile\n",
    "            tile = image[:, y*tile_size:(y+1)*tile_size, x*tile_size:(x+1)*tile_size]\n",
    "\n",
    "            # Check if the tile should be rotated 90 or -90 degrees (checker pattern)\n",
    "            if (x + y) % 2 == 0:\n",
    "                # Rotate 90 degrees\n",
    "                tile = tile.rot90(1, [1, 2])\n",
    "            else:\n",
    "                # Rotate -90 degrees\n",
    "                tile = tile.rot90(-1, [1, 2])\n",
    "\n",
    "            # Place the rotated tile back in the output tensor\n",
    "            output[:, y*tile_size:(y+1)*tile_size, x*tile_size:(x+1)*tile_size] = tile\n",
    "\n",
    "    return output\n",
    "\n",
    "learnable_image_a=lambda: image()\n",
    "learnable_image_b=lambda: rotate_tiles(image()) #Upside-down\n",
    "\n",
    "optim=torch.optim.SGD(image.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pKCQQg9teoMt"
   },
   "outputs": [],
   "source": [
    "labels=[label_a,label_b]\n",
    "learnable_images=[learnable_image_a,learnable_image_b]\n",
    "\n",
    "#The weight coefficients for each prompt. For example, if we have [0,1], then only the upside-down mode will be optimized\n",
    "weights=[1,1]\n",
    "\n",
    "weights=rp.as_numpy_array(weights)\n",
    "weights=weights/weights.sum()\n",
    "weights=weights*len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tWpLV_Toertv"
   },
   "outputs": [],
   "source": [
    "#For saving a timelapse\n",
    "ims=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "j6-FlVZPes09"
   },
   "outputs": [],
   "source": [
    "def get_display_image():\n",
    "    return rp.tiled_images(\n",
    "        [\n",
    "            rp.as_numpy_image(learnable_image_a()),\n",
    "            rp.as_numpy_image(learnable_image_b()),\n",
    "        ],\n",
    "        length=len(learnable_images),\n",
    "        border_thickness=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "bB-Uv4Y5et8J",
    "outputId": "0cf6d601-40a6-4140-f0c5-e4720d389822"
   },
   "outputs": [],
   "source": [
    "NUM_ITER=5000\n",
    "\n",
    "#Set the minimum and maximum noise timesteps for the dream loss (aka score distillation loss)\n",
    "s.max_step=MAX_STEP=990\n",
    "s.min_step=MIN_STEP=10 \n",
    "\n",
    "television = rp.JupyterDisplayChannel()\n",
    "television.display()\n",
    "\n",
    "display_eta=rp.eta(NUM_ITER, title='Status')\n",
    "\n",
    "DISPLAY_INTERVAL = 200\n",
    "\n",
    "print('Every %i iterations we display an image in the form [image_a, image_b], where'%DISPLAY_INTERVAL)\n",
    "print('    image_a = (the right-side up image)')\n",
    "print('    image_b = (image_a, but upside down)')\n",
    "print()\n",
    "print('Interrupt the kernel at any time to return the currently displayed image')\n",
    "print('You can run this cell again to resume training later on')\n",
    "print()\n",
    "print('Please expect this to take quite a while to get good images (especially on the slower Colab GPU\\'s)! The longer you wait the better they\\'ll be')\n",
    "\n",
    "try:\n",
    "    for iter_num in range(NUM_ITER):\n",
    "        display_eta(iter_num) #Print the remaining time\n",
    "\n",
    "        preds=[]\n",
    "        for label,learnable_image,weight,img_prompt in rp.random_batch(list(zip(labels,learnable_images,weights,img_prompts)), batch_size=1):\n",
    "            pred=s.train_step(\n",
    "                label.embedding,\n",
    "                learnable_image()[None],\n",
    "\n",
    "                #PRESETS (uncomment one):\n",
    "                noise_coef=.1*weight,guidance_scale=100,#10\n",
    "                # noise_coef=0,image_coef=-.01,guidance_scale=50,\n",
    "                # noise_coef=0,image_coef=-.005,guidance_scale=50,\n",
    "                # noise_coef=.1,image_coef=-.010,guidance_scale=50,\n",
    "                # noise_coef=.1,image_coef=-.005,guidance_scale=50,\n",
    "                # noise_coef=.1*weight, image_coef=-.005*weight, guidance_scale=50,\n",
    "                image_prompt=img_prompt,\n",
    "                img_prompts_coef=2.0*weight,\n",
    "            )\n",
    "            preds+=list(pred)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if iter_num and not iter_num%(DISPLAY_INTERVAL*50):\n",
    "                #Wipe the slate every 50 displays so they don't get cut off\n",
    "                from IPython.display import clear_output\n",
    "                clear_output()\n",
    "\n",
    "            if not iter_num%(DISPLAY_INTERVAL//4):\n",
    "                im = get_display_image()\n",
    "                ims.append(im)\n",
    "                television.update(im)\n",
    "                \n",
    "                if not iter_num%DISPLAY_INTERVAL:\n",
    "                    rp.display_image(im)\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "except KeyboardInterrupt:\n",
    "    print()\n",
    "    print('Interrupted early at iteration %i'%iter_num)\n",
    "    im = get_display_image()\n",
    "    ims.append(im)\n",
    "    rp.display_image(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwhrgJiie3mX"
   },
   "outputs": [],
   "source": [
    "print('Unsolved Image:')\n",
    "rp.display_image(rp.as_numpy_image(learnable_image_a()))\n",
    "\n",
    "print('Solved Image:')\n",
    "rp.display_image(rp.as_numpy_image(learnable_image_b()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YJJw4dXe4JJ"
   },
   "outputs": [],
   "source": [
    "folder = ''\n",
    "def save_run(name):\n",
    "    global folder\n",
    "    folder=\"untracked/parker_puzzle_runs/%s\"%name\n",
    "    if rp.path_exists(folder):\n",
    "        folder+='_%i'%time.time()\n",
    "    rp.make_directory(folder)\n",
    "    ims_names=['ims_%04i.png'%i for i in range(len(ims))]\n",
    "    with rp.SetCurrentDirectoryTemporarily(folder):\n",
    "        rp.save_images(ims,ims_names,show_progress=True)\n",
    "    print()\n",
    "    print('Saved timelapse to folder:',repr(folder))\n",
    "    \n",
    "save_run('-'.join([prompt_a,prompt_b])) #You can give it a good custom name if you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the twisting squares gif from the image\n",
    "def generate_twisting_squares_gif(input_path:str, output_path:str, num_frames:int = 31, duration:float = 0.1):\n",
    "    from PIL import Image\n",
    "    import imageio\n",
    "    \n",
    "    input_image = Image.open(input_path).convert('RGB')\n",
    "    width, height = input_image.size\n",
    "    input_image = input_image.crop((0, 0, width // 2, height))\n",
    "    width, height = input_image.size\n",
    "\n",
    "    # Divide the image into NUMBER_OF_SQUARES * NUMBER_OF_SQUARES parts\n",
    "    each_part = []\n",
    "    part_width = width // NUMBER_OF_SQUARES\n",
    "    part_height = height // NUMBER_OF_SQUARES\n",
    "    for i in range(NUMBER_OF_SQUARES):\n",
    "        for j in range(NUMBER_OF_SQUARES):\n",
    "            each_part.append(input_image.crop((j * part_width, i * part_height, (j + 1) * part_width, (i + 1) * part_height)))\n",
    "    \n",
    "    # make gif\n",
    "    frames = []\n",
    "    flag = 1  # determine the direction of rotation \n",
    "    for i in range(num_frames):\n",
    "        nowframe = Image.new('RGB', (width, height))\n",
    "        angle = i * (93 / num_frames)\n",
    "        for j in range(NUMBER_OF_SQUARES * NUMBER_OF_SQUARES):\n",
    "            rotated_part = each_part[j].rotate(flag * angle)\n",
    "            nowframe.paste(rotated_part, (j % NUMBER_OF_SQUARES * part_width, j // NUMBER_OF_SQUARES * part_height))\n",
    "            flag *= -1\n",
    "            if (NUMBER_OF_SQUARES % 2 == 0 and (j+1) % NUMBER_OF_SQUARES == 0):\n",
    "                flag *= -1\n",
    "        frames.append(nowframe)\n",
    "        if i in [0, num_frames - 1]:\n",
    "            frames.append(nowframe)\n",
    "            frames.append(nowframe)\n",
    "            frames.append(nowframe)\n",
    "    \n",
    "    imageio.mimsave(output_path, frames, duration=duration)\n",
    "    print(f'Twisting Squares images from {input_path} saved to {output_path} as gif')\n",
    "\n",
    "# If the path is wrong, please change the path by yourself\n",
    "read_path = 'ims_%04i.png'%(len(ims)-1)\n",
    "save_path = 'twisting_squares%04i.gif'%(len(ims)-1)\n",
    "with rp.SetCurrentDirectoryTemporarily(folder):\n",
    "    generate_twisting_squares_gif(read_path, save_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
